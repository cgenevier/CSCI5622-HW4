{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgenevier/CSCI5622-HW4/blob/part-d-DLmodels/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5628e982",
      "metadata": {
        "id": "5628e982"
      },
      "source": [
        "# Study 1: Designing explainable speech-based machine learning models of depression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-KMEnqTjpvj0",
      "metadata": {
        "id": "-KMEnqTjpvj0"
      },
      "source": [
        "To open this ipynb in Colab, click the \"Open in Colab\" button at the top of the ipynb in Github, or [follow this link](https://colab.research.google.com/github/cgenevier/CSCI5622-HW4/blob/main/main.ipynb).\n",
        "\n",
        "Given that Colab doesn't automatically load any of the content (data or other functions) from the Github repo, running the code below will copy the repo into the workspace directory for use. To save this ipynb file back to Github, select **File > Save** (which should show the repo if you're signed in) or **File > Save a copy in Github** if it's in the menu.\n",
        "\n",
        "Note that the content of the data files or any of the other file structures are not saved back to Github, so make sure that if you make changes to things there, that you put them in Github separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QzsrFmmJq1F6",
      "metadata": {
        "id": "QzsrFmmJq1F6"
      },
      "outputs": [],
      "source": [
        "# Clone Github Repo into the temporary local environment so data can be accessed and manipulated\n",
        "!git clone https://github.com/cgenevier/CSCI5622-HW4.git\n",
        "%cd CSCI5622-HW4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FBRtu3JYrGDy",
      "metadata": {
        "id": "FBRtu3JYrGDy"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# Helpers\n",
        "import glob\n",
        "\n",
        "# Pandas, seaborn, and numpy for data manipulation\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "import statistics as stat\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Keras & TensorFlow for building the neural networks\n",
        "import itertools, json, time\n",
        "from itertools import count\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks, backend as K\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Feature extraction\n",
        "!pip install vaderSentiment transformers torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import logging, BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Matplotlib for graphing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Disable progress bars (necessary for it to show up correctly in Github)\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Formatting the data - Depression Labels"
      ],
      "metadata": {
        "id": "3qF5pJFkBPzh"
      },
      "id": "3qF5pJFkBPzh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Depression Labels\n",
        "# Columns: Participant_ID, PHQ_Score\n",
        "depression_labels = pd.read_csv(\"data/DepressionLabels.csv\")\n",
        "\n",
        "# Rename Participant_ID to ParticipantID to match accoustic files below & force trimmed string type\n",
        "depression_labels = depression_labels.rename(columns={\"Participant_ID\": \"ParticipantID\"})\n",
        "depression_labels[\"ParticipantID\"] = depression_labels[\"ParticipantID\"].astype(str).str.strip()"
      ],
      "metadata": {
        "id": "jYBUArPBBVPr"
      },
      "id": "jYBUArPBBVPr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Formatting the data - Text Features"
      ],
      "metadata": {
        "id": "jvoV8lUqBa69"
      },
      "id": "jvoV8lUqBa69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kesV0RI-rd7I",
      "metadata": {
        "id": "kesV0RI-rd7I"
      },
      "outputs": [],
      "source": [
        "# Import Text Dataset (for text feature extraction)\n",
        "# Note: When comparing the E-DAIC_Transcripts files to the corresponding E-DAIC Acoustics files,\n",
        "# it looks like the transcripts sometimes only contain partial data from the accoustics text column -\n",
        "# for example, 386_Transcript.csv - so it seems to make sense to concatenate Text data in the\n",
        "# Acoustics file for completeness.\n",
        "rows = []\n",
        "for p in glob.glob(\"data/E-DAIC_Acoustics/*_utterance_agg.csv\"):\n",
        "    df = pd.read_csv(p)\n",
        "    df[\"ParticipantID\"] = df[\"ParticipantID\"].astype(str).str.strip()\n",
        "    full_text = \" \".join(df[\"Text\"].dropna().astype(str))\n",
        "    full_text = \" \".join(full_text.split())  # collapse whitespace\n",
        "    rows.append({\"ParticipantID\": df[\"ParticipantID\"].iloc[0], \"FullText\": full_text})\n",
        "\n",
        "# Columns: ParticipantID, FullText\n",
        "text_df = pd.DataFrame(rows)\n",
        "# Merge with labels. Columns: ParticipantID, FullText, PHQ_Score\n",
        "lang_df = depression_labels.merge(text_df, on=\"ParticipantID\", how=\"inner\")\n",
        "\n",
        "# Inspect results\n",
        "lang_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Formatting the data - Acoustic Features"
      ],
      "metadata": {
        "id": "XQzbiWE1Bfmd"
      },
      "id": "XQzbiWE1Bfmd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Accoustic Dataset (for part c, d)\n",
        "\n",
        "# Helper function for mean, standard dev, & IQR (interquartile range)\n",
        "def summarize_cols(num_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # mean and std per column (NaN-safe)\n",
        "    mean_s = num_df.mean(numeric_only=True)\n",
        "    std_s  = num_df.std(numeric_only=True)\n",
        "    # IQR = Q3 - Q1 per column\n",
        "    q75 = num_df.quantile(0.75, numeric_only=True)\n",
        "    q25 = num_df.quantile(0.25, numeric_only=True)\n",
        "    iqr_s = q75 - q25\n",
        "\n",
        "    # assemble into a tidy (feature, stat) table\n",
        "    stats = pd.concat(\n",
        "        {\"mean\": mean_s, \"std\": std_s, \"iqr\": iqr_s},\n",
        "        axis=1\n",
        "    )  # index=feature, columns=[mean,std,iqr]\n",
        "\n",
        "    # flatten to one row with columns like feature__mean\n",
        "    wide = stats.stack().to_frame().T\n",
        "    wide.columns = [f\"{feat}__{stat}\" for feat, stat in wide.columns]\n",
        "    return wide\n",
        "\n",
        "# Each file in E-DAIC_Acoustics contains utterance-level acoustic features for one participant.\n",
        "rows_with_conf = []\n",
        "rows_no_conf = []\n",
        "for p in glob.glob(\"data/E-DAIC_Acoustics/*_utterance_agg.csv\"):\n",
        "    df = pd.read_csv(p)\n",
        "    df[\"ParticipantID\"] = df[\"ParticipantID\"].astype(str).str.strip()\n",
        "\n",
        "    # Include Confidence (column 5) + all acoustic features (6+)\n",
        "    numeric_with_conf = df.columns[5:]\n",
        "    df[numeric_with_conf] = df[numeric_with_conf].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    agg_with_conf = summarize_cols(df[numeric_with_conf])\n",
        "    agg_with_conf.insert(0, \"ParticipantID\", df[\"ParticipantID\"].iloc[0])\n",
        "    rows_with_conf.append(agg_with_conf)\n",
        "\n",
        "    # Excludes Confidence - only include acoustic features\n",
        "    numeric_no_conf = df.columns[6:]\n",
        "    df[numeric_no_conf] = df[numeric_no_conf].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    agg_no_conf = summarize_cols(df[numeric_no_conf])\n",
        "    agg_no_conf.insert(0, \"ParticipantID\", df[\"ParticipantID\"].iloc[0])\n",
        "    rows_no_conf.append(agg_no_conf)\n",
        "\n",
        "# Combine into one DataFrame each\n",
        "acoustic_features_with_conf = pd.concat(rows_with_conf, ignore_index=True)\n",
        "acoustic_features_no_conf = pd.concat(rows_no_conf, ignore_index=True)\n",
        "\n",
        "# Merge with labels to add PHQ_Score\n",
        "acoustic_df_with_confidence = depression_labels.merge(acoustic_features_with_conf, on=\"ParticipantID\", how=\"inner\")\n",
        "acoustic_df_no_confidence   = depression_labels.merge(acoustic_features_no_conf, on=\"ParticipantID\", how=\"inner\")\n",
        "\n",
        "# Reorder columns: ParticipantID, PHQ_Score, then features\n",
        "cols = [\"ParticipantID\", \"PHQ_Score\"] + [c for c in acoustic_df_with_confidence.columns if c not in [\"ParticipantID\", \"PHQ_Score\"]]\n",
        "acoustic_df_with_confidence = acoustic_df_with_confidence[cols]\n",
        "cols = [\"ParticipantID\", \"PHQ_Score\"] + [c for c in acoustic_df_no_confidence.columns if c not in [\"ParticipantID\", \"PHQ_Score\"]]\n",
        "acoustic_df_no_confidence = acoustic_df_no_confidence[cols]\n",
        "\n",
        "# Inspect results\n",
        "display(acoustic_df_with_confidence.head())\n",
        "display(acoustic_df_no_confidence.head())"
      ],
      "metadata": {
        "id": "o8gRSINXBmUJ"
      },
      "id": "o8gRSINXBmUJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bb4bfe84",
      "metadata": {
        "id": "bb4bfe84"
      },
      "source": [
        "### (a) (2 points) Extracting language features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cc1fb7",
      "metadata": {
        "id": "c7cc1fb7"
      },
      "source": [
        "**Syntactic vectorizers:** count vectorizer (e.g., CountVectorizer from sklearn) transforming\n",
        "a collection of text documents into a numerical matrix of word or token counts; TFIDF vectorizer (e.g., TfidfVectorizer from sklearn) incorporating document-level weighting,\n",
        "which emphasizes words significant to specific documentsâ€™ part-of-speech features counting\n",
        "the distribution of part of speech tags over a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd57d88c",
      "metadata": {
        "id": "dd57d88c"
      },
      "outputs": [],
      "source": [
        "# Use TfidfVectorizer from sklearn\n",
        "vect = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = vect.fit_transform(lang_df[\"FullText\"])\n",
        "\n",
        "# Convert sparse matrix to DataFrame\n",
        "syntactic_df = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=vect.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Add ParticipantID column & move to first column\n",
        "syntactic_df[\"ParticipantID\"] = lang_df[\"ParticipantID\"].values\n",
        "cols = [\"ParticipantID\"] + [c for c in syntactic_df.columns if c != \"ParticipantID\"]\n",
        "syntactic_df = syntactic_df[cols]\n",
        "\n",
        "# Add back in PHQ_Score & move to second column\n",
        "syntactic_df = syntactic_df.merge(depression_labels, on=\"ParticipantID\", how=\"inner\")\n",
        "cols = [\"ParticipantID\", \"PHQ_Score\"] + [c for c in syntactic_df.columns if c not in [\"ParticipantID\", \"PHQ_Score\"]]\n",
        "syntactic_df = syntactic_df[cols]\n",
        "\n",
        "# Inspect dataframe\n",
        "syntactic_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4662aca0",
      "metadata": {
        "id": "4662aca0"
      },
      "source": [
        "**Semantic features:** sentiment scores (e.g., Vader, https://github.com/cjhutto/vaderSentiment),\n",
        "topic distribution (using topic modeling), or named entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb764609",
      "metadata": {
        "id": "fb764609"
      },
      "outputs": [],
      "source": [
        "# Using Vader to analyze sentiment of the text data\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply Vader to the text data (creates 4 new columns)\n",
        "vader_scores = lang_df[\"FullText\"].apply(lambda x: pd.Series(analyzer.polarity_scores(str(x))))\n",
        "semantic_df = pd.concat([lang_df, vader_scores], axis=1)\n",
        "\n",
        "# Inspect dataframe\n",
        "semantic_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a6fd00",
      "metadata": {
        "id": "16a6fd00"
      },
      "source": [
        "**Advanced features:** word embeddings, such as Word2Vec or BERT (e.g., pytorch-pretrainedbert)) for capturing contextual meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4a1f0a",
      "metadata": {
        "id": "8a4a1f0a"
      },
      "outputs": [],
      "source": [
        "# Use BERT to capture contextual meaning (note: takes about 4 minutes to run on T4)\n",
        "\n",
        "# Load uncased base model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "# Loop through text data and get embeddings\n",
        "embeddings = []\n",
        "for text in lang_df[\"FullText\"]:\n",
        "    # Truncate long text (BERT max = 512 tokens)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # [CLS] token\n",
        "    embeddings.append(cls_embedding)\n",
        "\n",
        "# Convert list of embeddings (each 768-dim) to DataFrame\n",
        "bert_df = pd.DataFrame(np.vstack(embeddings))\n",
        "bert_df.columns = [f\"bert_{i}\" for i in range(bert_df.shape[1])]\n",
        "\n",
        "# Add ParticipantID and PHQ_Score\n",
        "bert_df = pd.concat([lang_df[[\"ParticipantID\", \"PHQ_Score\"]].reset_index(drop=True), bert_df], axis=1)\n",
        "\n",
        "# Inspect dataframe\n",
        "bert_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combined dataset:** Combined the three dataframes above into one with all the features"
      ],
      "metadata": {
        "id": "6wo7ZsVkGs2R"
      },
      "id": "6wo7ZsVkGs2R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all three on ParticipantID\n",
        "text_feature_df = (\n",
        "    syntactic_df\n",
        "    .merge(semantic_df, on=[\"ParticipantID\", \"PHQ_Score\"], how=\"outer\")\n",
        "    .merge(bert_df, on=[\"ParticipantID\", \"PHQ_Score\"], how=\"outer\")\n",
        ")\n",
        "text_feature_df.head()\n",
        "\n",
        "# Optional: sort by ParticipantID for clarity\n",
        "#merged_df = merged_df.sort_values(\"ParticipantID\").reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uk3qdhqGGzo6"
      },
      "id": "uk3qdhqGGzo6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9be58d95",
      "metadata": {
        "id": "9be58d95"
      },
      "source": [
        "### (b) (2 points) Estimating depression severity with interpretable models using language features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: In semantic feature extraction above, there are four features generated: neg, neu, pos, compound. They are inter-related because neg is the proportion of the document that is negative, neu is the proportion of the document that is neutral, pos is the proportion of the document that is positive, and compound is a normalized sentiment value that takes into account all 3. Should we remove some of these features because they're redundant?"
      ],
      "metadata": {
        "id": "6tZemAZTQCAy"
      },
      "id": "6tZemAZTQCAy"
    },
    {
      "cell_type": "markdown",
      "id": "e6a8a6a4",
      "metadata": {
        "id": "e6a8a6a4"
      },
      "source": [
        "### (c) (2 points) Estimating depression severity with interpretable models using acoustic features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cea9a9b",
      "metadata": {
        "id": "2cea9a9b"
      },
      "source": [
        "### (d) (2 points) Estimating depression severity with unimodal and multimodal deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "QV9JHd7gL0HG",
        "outputId": "23945f01-81c9-4556-ed22-e47dc358bc1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QV9JHd7gL0HG",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Helpers\"\"\"\n",
        "\n",
        "# PyTorch dataset wrapper for tabular (numpy) features\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]),\n",
        "            torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# Basic MLP for regression\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "def compute_metrics(y_true, y_pred, global_max_phq):\n",
        "    y_true = np.asarray(y_true, dtype=np.float32)\n",
        "    y_pred = np.asarray(y_pred, dtype=np.float32)\n",
        "\n",
        "    # Pearson correlation\n",
        "    r, _ = pearsonr(y_true, y_pred)\n",
        "\n",
        "    # Absolute relative error (average over participants)\n",
        "    re = np.mean(np.abs(y_pred - y_true) / global_max_phq)\n",
        "\n",
        "    return r, re\n",
        "\n",
        "# Train the MLP on one fold of cross-validation\n",
        "def train_one_fold(X_train, y_train, X_val, y_val, num_epochs=10, batch_size=32, lr=1e-3):\n",
        "\n",
        "    # Regularization\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "    # Build PyTorch datasets/loaders\n",
        "    train_ds = TabularDataset(X_train_scaled.astype(np.float32), y_train)\n",
        "    val_ds   = TabularDataset(X_val_scaled.astype(np.float32), y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model, loss, optimizer\n",
        "    model = MLPRegressor(input_dim=X_train.shape[1]).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(Xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * len(Xb)\n",
        "\n",
        "        avg_loss = running_loss / len(train_ds)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_preds, val_true = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in val_loader:\n",
        "                Xb = Xb.to(device)\n",
        "                preds = model(Xb).cpu().numpy()\n",
        "                val_preds.append(preds)\n",
        "                val_true.append(yb.numpy())\n",
        "\n",
        "        # Concatenate predictions across batches\n",
        "        val_preds = np.concatenate(val_preds)\n",
        "        val_true  = np.concatenate(val_true)\n",
        "\n",
        "        # Compute metrics\n",
        "        global_max_phq = y.max()\n",
        "        r, re = compute_metrics(val_true, val_preds, global_max_phq)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1:02d} | train_loss={avg_loss:.4f} | val_r={r:.3f} | val_RE={re:.3f}\")\n",
        "\n",
        "    return model, scaler\n"
      ],
      "metadata": {
        "id": "4botZ5OSPHUC"
      },
      "id": "4botZ5OSPHUC",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Language features only\"\"\"\n",
        "\n",
        "df = text_feature_df.dropna(subset=[\"PHQ_Score\"]).copy() # Keep only rows with PHQ scores\n",
        "feature_cols = [c for c in text_feature_df.columns if c.startswith(\"bert_\")] # Use BERT features\n",
        "\n",
        "X = df[feature_cols].fillna(0.0).to_numpy().astype(np.float32) # Features\n",
        "y = df[\"PHQ_Score\"].to_numpy().astype(np.float32)              # Labels\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n",
        "all_val_preds = np.zeros_like(y, dtype=np.float32)    # Store predictions for all validation folds\n",
        "fold_results = []\n",
        "\n",
        "global_max_phq = y.max() # For relative error normalization\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f\"\\n=== Fold {fold+1} ===\")\n",
        "\n",
        "    # Split\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # Train on this fold\n",
        "    model, scaler = train_one_fold(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        num_epochs=10,\n",
        "        batch_size=32,\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    # Predict on validation set\n",
        "    X_val_scaled = scaler.transform(X_val).astype(np.float32)\n",
        "    val_ds = TabularDataset(X_val_scaled, y_val)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, _ in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            preds = model(Xb).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "\n",
        "    # Combine predictions across batches\n",
        "    val_preds = np.concatenate(val_preds).astype(np.float32)\n",
        "    all_val_preds[val_idx] = val_preds\n",
        "\n",
        "    # Compute metrics\n",
        "    r, re = compute_metrics(y_val, val_preds, global_max_phq)\n",
        "    fold_results.append((r, re))\n",
        "\n",
        "    # This fold results\n",
        "    print(f\"Fold {fold+1} summary: r={r:.3f}, RE={re:.3f}\")\n",
        "\n",
        "# Final results across all folds\n",
        "overall_r, overall_re = compute_metrics(y, all_val_preds, global_max_phq)\n",
        "print(\"\\n=== Overall 5-fold CV results (language-only MLP) ===\")\n",
        "print(f\"Pearson r: {overall_r:.3f}\")\n",
        "print(f\"Mean absolute relative error: {overall_re:.3f}\")\n",
        "print(\"Per-fold (r, RE):\", fold_results)\n"
      ],
      "metadata": {
        "id": "xN2TUOCUNdJU",
        "outputId": "b8765818-f016-47ad-cca0-569c11aae02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xN2TUOCUNdJU",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (134, 768)\n",
            "y shape: (134,)\n",
            "\n",
            "=== Fold 1 ===\n",
            "Epoch 01 | train_loss=67.5614 | val_r=-0.192 | val_RE=0.184\n",
            "Epoch 02 | train_loss=53.8038 | val_r=-0.121 | val_RE=0.170\n",
            "Epoch 03 | train_loss=43.9864 | val_r=-0.111 | val_RE=0.165\n",
            "Epoch 04 | train_loss=34.9317 | val_r=-0.107 | val_RE=0.162\n",
            "Epoch 05 | train_loss=27.5896 | val_r=-0.088 | val_RE=0.165\n",
            "Epoch 06 | train_loss=21.3995 | val_r=-0.068 | val_RE=0.173\n",
            "Epoch 07 | train_loss=18.3062 | val_r=-0.034 | val_RE=0.180\n",
            "Epoch 08 | train_loss=15.0482 | val_r=0.006 | val_RE=0.183\n",
            "Epoch 09 | train_loss=11.6023 | val_r=0.009 | val_RE=0.186\n",
            "Epoch 10 | train_loss=9.5905 | val_r=-0.008 | val_RE=0.187\n",
            "Fold 1 summary: r=-0.008, RE=0.187\n",
            "\n",
            "=== Fold 2 ===\n",
            "Epoch 01 | train_loss=62.3974 | val_r=-0.437 | val_RE=0.266\n",
            "Epoch 02 | train_loss=51.1440 | val_r=-0.468 | val_RE=0.250\n",
            "Epoch 03 | train_loss=41.6718 | val_r=-0.435 | val_RE=0.238\n",
            "Epoch 04 | train_loss=32.3926 | val_r=-0.398 | val_RE=0.230\n",
            "Epoch 05 | train_loss=25.6756 | val_r=-0.377 | val_RE=0.229\n",
            "Epoch 06 | train_loss=20.4052 | val_r=-0.362 | val_RE=0.230\n",
            "Epoch 07 | train_loss=15.2552 | val_r=-0.347 | val_RE=0.232\n",
            "Epoch 08 | train_loss=12.3143 | val_r=-0.340 | val_RE=0.234\n",
            "Epoch 09 | train_loss=9.9914 | val_r=-0.332 | val_RE=0.236\n",
            "Epoch 10 | train_loss=8.3520 | val_r=-0.327 | val_RE=0.241\n",
            "Fold 2 summary: r=-0.327, RE=0.241\n",
            "\n",
            "=== Fold 3 ===\n",
            "Epoch 01 | train_loss=50.8265 | val_r=-0.155 | val_RE=0.342\n",
            "Epoch 02 | train_loss=41.0565 | val_r=-0.089 | val_RE=0.324\n",
            "Epoch 03 | train_loss=32.9707 | val_r=-0.079 | val_RE=0.308\n",
            "Epoch 04 | train_loss=27.0438 | val_r=-0.060 | val_RE=0.296\n",
            "Epoch 05 | train_loss=20.3487 | val_r=-0.060 | val_RE=0.286\n",
            "Epoch 06 | train_loss=16.7707 | val_r=-0.087 | val_RE=0.281\n",
            "Epoch 07 | train_loss=13.9628 | val_r=-0.101 | val_RE=0.279\n",
            "Epoch 08 | train_loss=11.5839 | val_r=-0.136 | val_RE=0.282\n",
            "Epoch 09 | train_loss=9.3273 | val_r=-0.157 | val_RE=0.284\n",
            "Epoch 10 | train_loss=6.8916 | val_r=-0.148 | val_RE=0.285\n",
            "Fold 3 summary: r=-0.148, RE=0.285\n",
            "\n",
            "=== Fold 4 ===\n",
            "Epoch 01 | train_loss=74.0817 | val_r=0.122 | val_RE=0.113\n",
            "Epoch 02 | train_loss=60.4259 | val_r=0.068 | val_RE=0.102\n",
            "Epoch 03 | train_loss=49.9956 | val_r=0.050 | val_RE=0.104\n",
            "Epoch 04 | train_loss=41.6806 | val_r=0.035 | val_RE=0.111\n",
            "Epoch 05 | train_loss=32.1207 | val_r=0.021 | val_RE=0.122\n",
            "Epoch 06 | train_loss=24.8708 | val_r=0.008 | val_RE=0.147\n",
            "Epoch 07 | train_loss=20.2655 | val_r=0.007 | val_RE=0.172\n",
            "Epoch 08 | train_loss=15.3596 | val_r=-0.001 | val_RE=0.193\n",
            "Epoch 09 | train_loss=12.7633 | val_r=-0.012 | val_RE=0.209\n",
            "Epoch 10 | train_loss=9.8136 | val_r=-0.021 | val_RE=0.217\n",
            "Fold 4 summary: r=-0.021, RE=0.217\n",
            "\n",
            "=== Fold 5 ===\n",
            "Epoch 01 | train_loss=56.0794 | val_r=-0.095 | val_RE=0.288\n",
            "Epoch 02 | train_loss=43.4950 | val_r=-0.109 | val_RE=0.284\n",
            "Epoch 03 | train_loss=34.8193 | val_r=-0.117 | val_RE=0.282\n",
            "Epoch 04 | train_loss=26.3349 | val_r=-0.119 | val_RE=0.282\n",
            "Epoch 05 | train_loss=21.3940 | val_r=-0.139 | val_RE=0.285\n",
            "Epoch 06 | train_loss=15.1613 | val_r=-0.154 | val_RE=0.289\n",
            "Epoch 07 | train_loss=12.6936 | val_r=-0.165 | val_RE=0.292\n",
            "Epoch 08 | train_loss=9.9146 | val_r=-0.169 | val_RE=0.296\n",
            "Epoch 09 | train_loss=7.8437 | val_r=-0.163 | val_RE=0.298\n",
            "Epoch 10 | train_loss=6.2718 | val_r=-0.157 | val_RE=0.299\n",
            "Fold 5 summary: r=-0.157, RE=0.299\n",
            "\n",
            "=== Overall 5-fold CV results (language-only MLP) ===\n",
            "Pearson r: -0.213\n",
            "Mean absolute relative error: 0.246\n",
            "Per-fold (r, RE): [(np.float32(-0.008254568), np.float32(0.18720827)), (np.float32(-0.32664698), np.float32(0.24083404)), (np.float32(-0.14816754), np.float32(0.2852569)), (np.float32(-0.02099417), np.float32(0.21698071)), (np.float32(-0.15739919), np.float32(0.29931104))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Acoustic features only\"\"\"\n",
        "\n",
        "df = acoustic_df_no_confidence.dropna(subset=[\"PHQ_Score\"]).copy() # Keep only rows with PHQ scores\n",
        "non_feature_cols = [\"ParticipantID\", \"PHQ_Score\"]\n",
        "feature_cols = [c for c in df.columns if c not in non_feature_cols] # Input columns\n",
        "\n",
        "X = df[feature_cols].fillna(0.0).to_numpy().astype(np.float32) # Features\n",
        "y = df[\"PHQ_Score\"].to_numpy().astype(np.float32)              # Labels\n",
        "\n",
        "print(\"Acoustic X shape:\", X.shape)\n",
        "print(\"Acoustic y shape:\", y.shape)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n",
        "all_val_preds = np.zeros_like(y, dtype=np.float32)    # Store predictions for all validation folds\n",
        "fold_results = []\n",
        "\n",
        "global_max_phq = y.max() # For relative error normalization\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f\"\\n=== Acoustic Fold {fold+1} ===\")\n",
        "\n",
        "    # Split\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # Train on this fold\n",
        "    model, scaler = train_one_fold(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        num_epochs=10,\n",
        "        batch_size=32,\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    # Predict on validation set\n",
        "    X_val_scaled = scaler.transform(X_val).astype(np.float32)\n",
        "    val_ds = TabularDataset(X_val_scaled, y_val)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, _ in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            preds = model(Xb).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "\n",
        "    # Combine predictions across batches\n",
        "    val_preds = np.concatenate(val_preds).astype(np.float32)\n",
        "    all_val_preds[val_idx] = val_preds\n",
        "\n",
        "    # Compute metrics\n",
        "    r, re = compute_metrics(y_val, val_preds, global_max_phq)\n",
        "    fold_results.append((r, re))\n",
        "\n",
        "    # This fold results\n",
        "    print(f\"Fold {fold+1} summary: r={r:.3f}, RE={re:.3f}\")\n",
        "\n",
        "# Final results across all folds\n",
        "overall_r, overall_re = compute_metrics(y, all_val_preds, global_max_phq)\n",
        "print(\"\\n=== Overall 5-fold CV results (acoustic-only MLP) ===\")\n",
        "print(f\"Pearson r: {overall_r:.3f}\")\n",
        "print(f\"Mean absolute relative error: {overall_re:.3f}\")\n",
        "print(\"Per-fold (r, RE):\", fold_results)\n"
      ],
      "metadata": {
        "id": "UI3OWCysN2i8",
        "outputId": "12457806-9c3b-4b5e-d1b7-da652a7450a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UI3OWCysN2i8",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acoustic X shape: (134, 69)\n",
            "Acoustic y shape: (134,)\n",
            "\n",
            "=== Acoustic Fold 1 ===\n",
            "Epoch 01 | train_loss=68.4185 | val_r=0.139 | val_RE=0.193\n",
            "Epoch 02 | train_loss=63.7061 | val_r=0.094 | val_RE=0.183\n",
            "Epoch 03 | train_loss=59.8291 | val_r=0.039 | val_RE=0.175\n",
            "Epoch 04 | train_loss=56.1703 | val_r=0.001 | val_RE=0.169\n",
            "Epoch 05 | train_loss=52.6099 | val_r=-0.009 | val_RE=0.163\n",
            "Epoch 06 | train_loss=49.7511 | val_r=-0.014 | val_RE=0.159\n",
            "Epoch 07 | train_loss=46.3375 | val_r=-0.012 | val_RE=0.154\n",
            "Epoch 08 | train_loss=43.3980 | val_r=-0.016 | val_RE=0.152\n",
            "Epoch 09 | train_loss=41.1712 | val_r=-0.015 | val_RE=0.151\n",
            "Epoch 10 | train_loss=38.2755 | val_r=-0.015 | val_RE=0.152\n",
            "Fold 1 summary: r=-0.015, RE=0.152\n",
            "\n",
            "=== Acoustic Fold 2 ===\n",
            "Epoch 01 | train_loss=64.5845 | val_r=0.121 | val_RE=0.280\n",
            "Epoch 02 | train_loss=60.8403 | val_r=0.070 | val_RE=0.265\n",
            "Epoch 03 | train_loss=56.8003 | val_r=0.049 | val_RE=0.253\n",
            "Epoch 04 | train_loss=53.2129 | val_r=0.042 | val_RE=0.241\n",
            "Epoch 05 | train_loss=50.1440 | val_r=0.025 | val_RE=0.232\n",
            "Epoch 06 | train_loss=47.1002 | val_r=0.023 | val_RE=0.224\n",
            "Epoch 07 | train_loss=44.4569 | val_r=0.027 | val_RE=0.216\n",
            "Epoch 08 | train_loss=41.7182 | val_r=0.032 | val_RE=0.210\n",
            "Epoch 09 | train_loss=39.8432 | val_r=0.037 | val_RE=0.206\n",
            "Epoch 10 | train_loss=37.5696 | val_r=0.047 | val_RE=0.203\n",
            "Fold 2 summary: r=0.047, RE=0.203\n",
            "\n",
            "=== Acoustic Fold 3 ===\n",
            "Epoch 01 | train_loss=52.8952 | val_r=-0.040 | val_RE=0.356\n",
            "Epoch 02 | train_loss=49.1921 | val_r=-0.019 | val_RE=0.344\n",
            "Epoch 03 | train_loss=45.4062 | val_r=0.000 | val_RE=0.332\n",
            "Epoch 04 | train_loss=42.7157 | val_r=0.011 | val_RE=0.321\n",
            "Epoch 05 | train_loss=40.0503 | val_r=0.016 | val_RE=0.311\n",
            "Epoch 06 | train_loss=36.9596 | val_r=0.025 | val_RE=0.301\n",
            "Epoch 07 | train_loss=35.2644 | val_r=0.050 | val_RE=0.292\n",
            "Epoch 08 | train_loss=33.7423 | val_r=0.070 | val_RE=0.285\n",
            "Epoch 09 | train_loss=31.1817 | val_r=0.078 | val_RE=0.279\n",
            "Epoch 10 | train_loss=29.4683 | val_r=0.095 | val_RE=0.274\n",
            "Fold 3 summary: r=0.095, RE=0.274\n",
            "\n",
            "=== Acoustic Fold 4 ===\n",
            "Epoch 01 | train_loss=75.4112 | val_r=-0.113 | val_RE=0.132\n",
            "Epoch 02 | train_loss=70.1363 | val_r=-0.053 | val_RE=0.124\n",
            "Epoch 03 | train_loss=65.8837 | val_r=-0.012 | val_RE=0.117\n",
            "Epoch 04 | train_loss=61.7905 | val_r=0.007 | val_RE=0.113\n",
            "Epoch 05 | train_loss=58.3022 | val_r=0.019 | val_RE=0.111\n",
            "Epoch 06 | train_loss=54.8709 | val_r=0.031 | val_RE=0.110\n",
            "Epoch 07 | train_loss=51.3671 | val_r=0.043 | val_RE=0.111\n",
            "Epoch 08 | train_loss=49.1290 | val_r=0.054 | val_RE=0.112\n",
            "Epoch 09 | train_loss=45.0768 | val_r=0.068 | val_RE=0.114\n",
            "Epoch 10 | train_loss=42.8867 | val_r=0.083 | val_RE=0.117\n",
            "Fold 4 summary: r=0.083, RE=0.117\n",
            "\n",
            "=== Acoustic Fold 5 ===\n",
            "Epoch 01 | train_loss=56.7237 | val_r=-0.069 | val_RE=0.290\n",
            "Epoch 02 | train_loss=52.3347 | val_r=-0.130 | val_RE=0.287\n",
            "Epoch 03 | train_loss=47.9861 | val_r=-0.143 | val_RE=0.283\n",
            "Epoch 04 | train_loss=44.8727 | val_r=-0.140 | val_RE=0.281\n",
            "Epoch 05 | train_loss=41.7417 | val_r=-0.141 | val_RE=0.281\n",
            "Epoch 06 | train_loss=39.3672 | val_r=-0.134 | val_RE=0.281\n",
            "Epoch 07 | train_loss=36.5501 | val_r=-0.129 | val_RE=0.281\n",
            "Epoch 08 | train_loss=34.4771 | val_r=-0.120 | val_RE=0.282\n",
            "Epoch 09 | train_loss=32.0346 | val_r=-0.111 | val_RE=0.283\n",
            "Epoch 10 | train_loss=29.9771 | val_r=-0.098 | val_RE=0.284\n",
            "Fold 5 summary: r=-0.098, RE=0.284\n",
            "\n",
            "=== Overall 5-fold CV results (acoustic-only MLP) ===\n",
            "Pearson r: -0.036\n",
            "Mean absolute relative error: 0.205\n",
            "Per-fold (r, RE): [(np.float32(-0.015259682), np.float32(0.15153247)), (np.float32(0.04674346), np.float32(0.20327975)), (np.float32(0.09508416), np.float32(0.27374658)), (np.float32(0.08293656), np.float32(0.11661835)), (np.float32(-0.098113686), np.float32(0.28427246))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Multimodal (language and acoustic features)\"\"\"\n",
        "\n",
        "# Merge acoustic and language features\n",
        "multi_df = (\n",
        "    text_feature_df\n",
        "    .merge(acoustic_df_no_confidence, on=[\"ParticipantID\", \"PHQ_Score\"], how=\"inner\")\n",
        ")\n",
        "\n",
        "df = multi_df.dropna(subset=[\"PHQ_Score\"]).copy() # Keep only rows with PHQ scores\n",
        "\n",
        "lang_cols = [c for c in multi_df.columns if c.startswith(\"bert_\")]\n",
        "acoustic_cols = [c for c in multi_df.columns\n",
        "                 if c not in [\"ParticipantID\", \"PHQ_Score\", \"FullText\"]\n",
        "                 and not c.startswith(\"bert_\")] # Avoid double-counting language features\n",
        "feature_cols = lang_cols + acoustic_cols # Concatenate both sets\n",
        "\n",
        "X = df[feature_cols].fillna(0.0).to_numpy().astype(np.float32) # Features\n",
        "y = df[\"PHQ_Score\"].to_numpy().astype(np.float32)              # Labels\n",
        "\n",
        "print(\"Multimodal X shape:\", X.shape)\n",
        "print(\"Multimodal y shape:\", y.shape)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n",
        "all_val_preds = np.zeros_like(y, dtype=np.float32)    # Store predictions for all validation folds\n",
        "fold_results = []\n",
        "\n",
        "global_max_phq = y.max() # For relative error normalization\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f\"\\n=== Multimodal Fold {fold+1} ===\")\n",
        "\n",
        "    # Split\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    # Train on this fold\n",
        "    model, scaler = train_one_fold(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        num_epochs=10,\n",
        "        batch_size=32,\n",
        "        lr=1e-3\n",
        "    )\n",
        "\n",
        "    # Predict on validation set\n",
        "    X_val_scaled = scaler.transform(X_val).astype(np.float32)\n",
        "    val_ds = TabularDataset(X_val_scaled, y_val)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, _ in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            preds = model(Xb).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "\n",
        "    # Combine predictions across batches\n",
        "    val_preds = np.concatenate(val_preds).astype(np.float32)\n",
        "    all_val_preds[val_idx] = val_preds\n",
        "\n",
        "    # Compute metrics\n",
        "    r, re = compute_metrics(y_val, val_preds, global_max_phq)\n",
        "    fold_results.append((r, re))\n",
        "\n",
        "    # This fold results\n",
        "    print(f\"Fold {fold+1} summary: r={r:.3f}, RE={re:.3f}\")\n",
        "\n",
        "# Final results across all folds\n",
        "overall_r, overall_re = compute_metrics(y, all_val_preds, global_max_phq)\n",
        "print(\"\\n=== Overall 5-fold CV results (multimodal MLP) ===\")\n",
        "print(f\"Pearson r: {overall_r:.3f}\")\n",
        "print(f\"Mean absolute relative error: {overall_re:.3f}\")\n",
        "print(\"Per-fold (r, RE):\", fold_results)"
      ],
      "metadata": {
        "id": "7P7I8EL4OMPx",
        "outputId": "ebc42b72-3f6d-46fa-fb26-81deed13df01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7P7I8EL4OMPx",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multimodal X shape: (134, 1841)\n",
            "Multimodal y shape: (134,)\n",
            "\n",
            "=== Multimodal Fold 1 ===\n",
            "Epoch 01 | train_loss=71.1891 | val_r=-0.338 | val_RE=0.198\n",
            "Epoch 02 | train_loss=50.1982 | val_r=-0.137 | val_RE=0.184\n",
            "Epoch 03 | train_loss=34.7170 | val_r=-0.073 | val_RE=0.171\n",
            "Epoch 04 | train_loss=21.4074 | val_r=-0.028 | val_RE=0.159\n",
            "Epoch 05 | train_loss=12.2882 | val_r=0.001 | val_RE=0.156\n",
            "Epoch 06 | train_loss=6.6352 | val_r=0.034 | val_RE=0.162\n",
            "Epoch 07 | train_loss=4.3216 | val_r=0.076 | val_RE=0.165\n",
            "Epoch 08 | train_loss=3.1201 | val_r=0.107 | val_RE=0.165\n",
            "Epoch 09 | train_loss=2.6576 | val_r=0.116 | val_RE=0.163\n",
            "Epoch 10 | train_loss=2.2221 | val_r=0.103 | val_RE=0.162\n",
            "Fold 1 summary: r=0.103, RE=0.162\n",
            "\n",
            "=== Multimodal Fold 2 ===\n",
            "Epoch 01 | train_loss=61.8432 | val_r=-0.096 | val_RE=0.263\n",
            "Epoch 02 | train_loss=43.2430 | val_r=-0.172 | val_RE=0.249\n",
            "Epoch 03 | train_loss=30.6390 | val_r=-0.199 | val_RE=0.242\n",
            "Epoch 04 | train_loss=17.8422 | val_r=-0.203 | val_RE=0.234\n",
            "Epoch 05 | train_loss=11.2538 | val_r=-0.186 | val_RE=0.227\n",
            "Epoch 06 | train_loss=4.7888 | val_r=-0.180 | val_RE=0.222\n",
            "Epoch 07 | train_loss=2.9935 | val_r=-0.170 | val_RE=0.221\n",
            "Epoch 08 | train_loss=2.5160 | val_r=-0.149 | val_RE=0.221\n",
            "Epoch 09 | train_loss=1.9602 | val_r=-0.135 | val_RE=0.221\n",
            "Epoch 10 | train_loss=1.7321 | val_r=-0.126 | val_RE=0.223\n",
            "Fold 2 summary: r=-0.126, RE=0.223\n",
            "\n",
            "=== Multimodal Fold 3 ===\n",
            "Epoch 01 | train_loss=50.8916 | val_r=0.168 | val_RE=0.341\n",
            "Epoch 02 | train_loss=32.5661 | val_r=0.156 | val_RE=0.321\n",
            "Epoch 03 | train_loss=20.2190 | val_r=0.128 | val_RE=0.299\n",
            "Epoch 04 | train_loss=11.8841 | val_r=0.090 | val_RE=0.282\n",
            "Epoch 05 | train_loss=6.0167 | val_r=0.054 | val_RE=0.275\n",
            "Epoch 06 | train_loss=3.4973 | val_r=0.035 | val_RE=0.274\n",
            "Epoch 07 | train_loss=2.4159 | val_r=0.015 | val_RE=0.275\n",
            "Epoch 08 | train_loss=2.3914 | val_r=0.011 | val_RE=0.277\n",
            "Epoch 09 | train_loss=1.3866 | val_r=0.012 | val_RE=0.282\n",
            "Epoch 10 | train_loss=1.7027 | val_r=0.022 | val_RE=0.284\n",
            "Fold 3 summary: r=0.022, RE=0.284\n",
            "\n",
            "=== Multimodal Fold 4 ===\n",
            "Epoch 01 | train_loss=75.2581 | val_r=-0.013 | val_RE=0.122\n",
            "Epoch 02 | train_loss=54.5236 | val_r=0.034 | val_RE=0.112\n",
            "Epoch 03 | train_loss=38.6784 | val_r=0.092 | val_RE=0.104\n",
            "Epoch 04 | train_loss=23.6214 | val_r=0.110 | val_RE=0.107\n",
            "Epoch 05 | train_loss=13.5823 | val_r=0.126 | val_RE=0.121\n",
            "Epoch 06 | train_loss=7.1042 | val_r=0.131 | val_RE=0.146\n",
            "Epoch 07 | train_loss=4.4381 | val_r=0.139 | val_RE=0.162\n",
            "Epoch 08 | train_loss=2.9441 | val_r=0.143 | val_RE=0.167\n",
            "Epoch 09 | train_loss=2.5124 | val_r=0.137 | val_RE=0.164\n",
            "Epoch 10 | train_loss=3.0198 | val_r=0.135 | val_RE=0.153\n",
            "Fold 4 summary: r=0.135, RE=0.153\n",
            "\n",
            "=== Multimodal Fold 5 ===\n",
            "Epoch 01 | train_loss=57.8967 | val_r=0.045 | val_RE=0.288\n",
            "Epoch 02 | train_loss=40.3747 | val_r=0.021 | val_RE=0.285\n",
            "Epoch 03 | train_loss=28.0530 | val_r=-0.008 | val_RE=0.281\n",
            "Epoch 04 | train_loss=15.9844 | val_r=-0.012 | val_RE=0.279\n",
            "Epoch 05 | train_loss=8.1407 | val_r=-0.026 | val_RE=0.278\n",
            "Epoch 06 | train_loss=4.6115 | val_r=-0.035 | val_RE=0.279\n",
            "Epoch 07 | train_loss=2.2542 | val_r=-0.041 | val_RE=0.279\n",
            "Epoch 08 | train_loss=2.5873 | val_r=-0.034 | val_RE=0.278\n",
            "Epoch 09 | train_loss=2.1259 | val_r=-0.032 | val_RE=0.278\n",
            "Epoch 10 | train_loss=1.7215 | val_r=-0.016 | val_RE=0.276\n",
            "Fold 5 summary: r=-0.016, RE=0.276\n",
            "\n",
            "=== Overall 5-fold CV results (multimodal MLP) ===\n",
            "Pearson r: -0.077\n",
            "Mean absolute relative error: 0.219\n",
            "Per-fold (r, RE): [(np.float32(0.10338626), np.float32(0.16150449)), (np.float32(-0.12583311), np.float32(0.22298194)), (np.float32(0.021689707), np.float32(0.28362012)), (np.float32(0.13506368), np.float32(0.15282238)), (np.float32(-0.015624047), np.float32(0.27641147))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4299e73c",
      "metadata": {
        "id": "4299e73c"
      },
      "source": [
        "### (e) (2 points) Explainable ML."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32cd773d",
      "metadata": {
        "id": "32cd773d"
      },
      "source": [
        "### (f) (Bonus, 2 points) Experimenting with transformers."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgenevier/CSCI5622-HW4/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5628e982",
      "metadata": {
        "id": "5628e982"
      },
      "source": [
        "# Study 1: Designing explainable speech-based machine learning models of depression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-KMEnqTjpvj0",
      "metadata": {
        "id": "-KMEnqTjpvj0"
      },
      "source": [
        "To open this ipynb in Colab, click the \"Open in Colab\" button at the top of the ipynb in Github, or [follow this link](https://colab.research.google.com/github/cgenevier/CSCI5622-HW4/blob/main/main.ipynb).\n",
        "\n",
        "Given that Colab doesn't automatically load any of the content (data or other functions) from the Github repo, running the code below will copy the repo into the workspace directory for use. To save this ipynb file back to Github, select **File > Save** (which should show the repo if you're signed in) or **File > Save a copy in Github** if it's in the menu.\n",
        "\n",
        "Note that the content of the data files or any of the other file structures are not saved back to Github, so make sure that if you make changes to things there, that you put them in Github separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "QzsrFmmJq1F6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsrFmmJq1F6",
        "outputId": "0e0e0353-819a-4ad4-91b4-2c952c8f3dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CSCI5622-HW4'...\n",
            "remote: Enumerating objects: 406, done.\u001b[K\n",
            "remote: Counting objects: 100% (406/406), done.\u001b[K\n",
            "remote: Compressing objects: 100% (403/403), done.\u001b[K\n",
            "remote: Total 406 (delta 10), reused 391 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (406/406), 5.14 MiB | 5.55 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/Library/WebServer/Sites/School/CSCI5622-HW4/CSCI5622-HW4\n"
          ]
        }
      ],
      "source": [
        "# Clone Github Repo into the temporary local environment so data can be accessed and manipulated\n",
        "!git clone https://github.com/cgenevier/CSCI5622-HW4.git\n",
        "%cd CSCI5622-HW4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "FBRtu3JYrGDy",
      "metadata": {
        "id": "FBRtu3JYrGDy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
            "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.9.0)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from vaderSentiment) (2.32.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# Helpers\n",
        "import glob\n",
        "\n",
        "# Pandas, seaborn, and numpy for data manipulation\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "import statistics as stat\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Keras & TensorFlow for building the neural networks\n",
        "import itertools, json, time\n",
        "from itertools import count\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks, backend as K\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Feature extraction\n",
        "!pip install vaderSentiment transformers torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Matplotlib for graphing\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "kesV0RI-rd7I",
      "metadata": {
        "id": "kesV0RI-rd7I"
      },
      "outputs": [],
      "source": [
        "# Import Depression Labels\n",
        "# Columns: Participant_ID, PHQ_Score\n",
        "depression_labels = pd.read_csv(\"data/DepressionLabels.csv\")\n",
        "\n",
        "# Rename Participant_ID to ParticipantID to match accoustic files below & force trimmed string type\n",
        "depression_labels = depression_labels.rename(columns={\"Participant_ID\": \"ParticipantID\"})\n",
        "depression_labels[\"ParticipantID\"] = depression_labels[\"ParticipantID\"].astype(str).str.strip()\n",
        "\n",
        "# Import Text Dataset (for text feature extraction)\n",
        "# Note: When comparing the E-DAIC_Transcripts files to the corresponding E-DAIC Acoustics files,\n",
        "# it looks like the transcripts sometimes only contain partial data from the accoustics text column -\n",
        "# for example, 386_Transcript.csv - so it seems to make sense to concatenate Text data in the\n",
        "# Acoustics file for completeness.\n",
        "rows = []\n",
        "for p in glob.glob(\"data/E-DAIC_Acoustics/*_utterance_agg.csv\"):\n",
        "    df = pd.read_csv(p)\n",
        "    df[\"ParticipantID\"] = df[\"ParticipantID\"].astype(str).str.strip()\n",
        "    full_text = \" \".join(df[\"Text\"].dropna().astype(str))\n",
        "    full_text = \" \".join(full_text.split())  # collapse whitespace\n",
        "    rows.append({\"ParticipantID\": df[\"ParticipantID\"].iloc[0], \"FullText\": full_text})\n",
        "\n",
        "# Columns: ParticipantID, FullText\n",
        "text_df = pd.DataFrame(rows)\n",
        "# Merge with labels. Columns: ParticipantID, FullText, PHQ_Score\n",
        "lang_df = depression_labels.merge(text_df, on=\"ParticipantID\", how=\"inner\")\n",
        "\n",
        "\n",
        "# Import Accoustic Dataset (for part c, d)\n",
        "# Note: May need to aggregate the data per participant, ex: mean, std, iqr for each feature\n",
        "# @TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4bfe84",
      "metadata": {
        "id": "bb4bfe84"
      },
      "source": [
        "### (a) (2 points) Extracting language features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cc1fb7",
      "metadata": {
        "id": "c7cc1fb7"
      },
      "source": [
        "**Syntactic vectorizers:** count vectorizer (e.g., CountVectorizer from sklearn) transforming\n",
        "a collection of text documents into a numerical matrix of word or token counts; TFIDF vectorizer (e.g., TfidfVectorizer from sklearn) incorporating document-level weighting,\n",
        "which emphasizes words significant to specific documents’ part-of-speech features counting\n",
        "the distribution of part of speech tags over a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dd57d88c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "dd57d88c",
        "outputId": "b68149e2-13de-47f0-9e21-8a88b3732c83"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ParticipantID</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>10</th>\n",
              "      <th>12</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>30</th>\n",
              "      <th>...</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngest</th>\n",
              "      <th>your</th>\n",
              "      <th>yourself</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>386</td>\n",
              "      <td>11</td>\n",
              "      <td>0.008627</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.352474</td>\n",
              "      <td>0.011232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016914</td>\n",
              "      <td>0.019322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>387</td>\n",
              "      <td>2</td>\n",
              "      <td>0.027755</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047486</td>\n",
              "      <td>0.346512</td>\n",
              "      <td>0.072275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.108838</td>\n",
              "      <td>0.031083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>388</td>\n",
              "      <td>17</td>\n",
              "      <td>0.031186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020382</td>\n",
              "      <td>0.034926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>389</td>\n",
              "      <td>14</td>\n",
              "      <td>0.054573</td>\n",
              "      <td>0.052964</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.325182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089168</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>390</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016775</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008948</td>\n",
              "      <td>0.020119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009498</td>\n",
              "      <td>0.016275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1002 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  ParticipantID  PHQ_Score        10        12   15   16   18   19        20  \\\n",
              "0           386         11  0.008627  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
              "1           387          2  0.027755  0.000000  0.0  0.0  0.0  0.0  0.032038   \n",
              "2           388         17  0.031186  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
              "3           389         14  0.054573  0.052964  0.0  0.0  0.0  0.0  0.000000   \n",
              "4           390          9  0.000000  0.000000  0.0  0.0  0.0  0.0  0.016775   \n",
              "\n",
              "    30  ...       yes  yesterday       yet      york       you     young  \\\n",
              "0  0.0  ...  0.010623   0.000000  0.000000  0.000000  0.352474  0.011232   \n",
              "1  0.0  ...  0.000000   0.000000  0.000000  0.047486  0.346512  0.072275   \n",
              "2  0.0  ...  0.038404   0.000000  0.040133  0.000000  0.283166  0.000000   \n",
              "3  0.0  ...  0.000000   0.000000  0.000000  0.000000  0.325182  0.000000   \n",
              "4  0.0  ...  0.008948   0.020119  0.000000  0.000000  0.123706  0.000000   \n",
              "\n",
              "   younger  youngest      your  yourself  \n",
              "0      0.0       0.0  0.016914  0.019322  \n",
              "1      0.0       0.0  0.108838  0.031083  \n",
              "2      0.0       0.0  0.020382  0.034926  \n",
              "3      0.0       0.0  0.089168  0.000000  \n",
              "4      0.0       0.0  0.009498  0.016275  \n",
              "\n",
              "[5 rows x 1002 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use TfidfVectorizer from sklearn\n",
        "vect = TfidfVectorizer(max_features=1000)\n",
        "X_tfidf = vect.fit_transform(lang_df[\"FullText\"])\n",
        "\n",
        "# Convert sparse matrix to DataFrame\n",
        "syntactic_df = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=vect.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Add ParticipantID column & move to first column\n",
        "syntactic_df[\"ParticipantID\"] = lang_df[\"ParticipantID\"].values\n",
        "cols = [\"ParticipantID\"] + [c for c in syntactic_df.columns if c != \"ParticipantID\"]\n",
        "syntactic_df = syntactic_df[cols]\n",
        "\n",
        "# Add back in PHQ_Score & move to second column\n",
        "syntactic_df = syntactic_df.merge(depression_labels, on=\"ParticipantID\", how=\"inner\")\n",
        "cols = [\"ParticipantID\", \"PHQ_Score\"] + [c for c in syntactic_df.columns if c not in [\"ParticipantID\", \"PHQ_Score\"]]\n",
        "syntactic_df = syntactic_df[cols]\n",
        "\n",
        "# Inspect dataframe\n",
        "syntactic_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4662aca0",
      "metadata": {
        "id": "4662aca0"
      },
      "source": [
        "**Semantic features:** sentiment scores (e.g., Vader, https://github.com/cjhutto/vaderSentiment),\n",
        "topic distribution (using topic modeling), or named entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fb764609",
      "metadata": {
        "id": "fb764609"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ParticipantID</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>FullText</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>386</td>\n",
              "      <td>11</td>\n",
              "      <td>might have pulled something that I'm going to ...</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.770</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.9999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>387</td>\n",
              "      <td>2</td>\n",
              "      <td>when she's done she'll let you know alrighty t...</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.9996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>388</td>\n",
              "      <td>17</td>\n",
              "      <td>are you okay with yes doing all right from Pas...</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.9953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>389</td>\n",
              "      <td>14</td>\n",
              "      <td>and please are you okay sure I'm okay small to...</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.9822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>390</td>\n",
              "      <td>9</td>\n",
              "      <td>and now she's going to chat with you for a bit...</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.9996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ParticipantID  PHQ_Score                                           FullText  \\\n",
              "0           386         11  might have pulled something that I'm going to ...   \n",
              "1           387          2  when she's done she'll let you know alrighty t...   \n",
              "2           388         17  are you okay with yes doing all right from Pas...   \n",
              "3           389         14  and please are you okay sure I'm okay small to...   \n",
              "4           390          9  and now she's going to chat with you for a bit...   \n",
              "\n",
              "     neg    neu    pos  compound  \n",
              "0  0.046  0.770  0.184    0.9999  \n",
              "1  0.050  0.665  0.285    0.9996  \n",
              "2  0.070  0.769  0.161    0.9953  \n",
              "3  0.057  0.827  0.116    0.9822  \n",
              "4  0.067  0.740  0.193    0.9996  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using Vader to analyze sentiment of the text data\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply Vader to the text data (creates 4 new columns)\n",
        "vader_scores = lang_df[\"FullText\"].apply(lambda x: pd.Series(analyzer.polarity_scores(str(x))))\n",
        "semantic_df = pd.concat([lang_df, vader_scores], axis=1)\n",
        "\n",
        "# Inspect dataframe\n",
        "semantic_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a6fd00",
      "metadata": {
        "id": "16a6fd00"
      },
      "source": [
        "**Advanced features:** word embeddings, such as Word2Vec or BERT (e.g., pytorch-pretrainedbert)) for capturing contextual meaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8a4a1f0a",
      "metadata": {
        "id": "8a4a1f0a"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use BERT to capture contextual meaning\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load uncased base model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Loop through text data and get embeddings\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2157\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 2157\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2126\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 2126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
            "\u001b[0;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
          ]
        }
      ],
      "source": [
        "# Use BERT to capture contextual meaning\n",
        "\n",
        "# Load uncased base model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval() \n",
        "\n",
        "# Loop through text data and get embeddings\n",
        "embeddings = []\n",
        "for text in lang_df[\"FullText\"]:\n",
        "    # Truncate long text (BERT max = 512 tokens)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # [CLS] token\n",
        "    embeddings.append(cls_embedding)\n",
        "\n",
        "# Convert list of embeddings (each 768-dim) to DataFrame\n",
        "bert_df = pd.DataFrame(np.vstack(embeddings))\n",
        "bert_df.columns = [f\"bert_{i}\" for i in range(bert_df.shape[1])]\n",
        "\n",
        "# Add ParticipantID and PHQ_Score\n",
        "bert_df = pd.concat([lang_df[[\"ParticipantID\", \"PHQ_Score\"]].reset_index(drop=True), bert_df], axis=1)\n",
        "\n",
        "# Inspect dataframe\n",
        "bert_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be58d95",
      "metadata": {
        "id": "9be58d95"
      },
      "source": [
        "### (b) (2 points) Estimating depression severity with interpretable models using language features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a8a6a4",
      "metadata": {
        "id": "e6a8a6a4"
      },
      "source": [
        "### (c) (2 points) Estimating depression severity with interpretable models using acoustic features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cea9a9b",
      "metadata": {
        "id": "2cea9a9b"
      },
      "source": [
        "### (d) (2 points) Estimating depression severity with unimodal and multimodal deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4299e73c",
      "metadata": {
        "id": "4299e73c"
      },
      "source": [
        "### (e) (2 points) Explainable ML."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32cd773d",
      "metadata": {
        "id": "32cd773d"
      },
      "source": [
        "### (f) (Bonus, 2 points) Experimenting with transformers."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
